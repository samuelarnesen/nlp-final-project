{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, SequentialSampler\n",
    "\n",
    "from bert_dataloader import get_wiki_data, get_fake_data\n",
    "from bert_models import BertMultiHeadModel # Custom model \n",
    "\n",
    "from transformers import BertModel, BertTokenizer, AdamW, BertPreTrainedModel, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug Models.ipynb        bert_dataloader.py        models.py\r\n",
      "Huggingface Bert.ipynb    bert_models.py            \u001b[1m\u001b[34mprecomputed\u001b[m\u001b[m/\r\n",
      "Multi Head model.ipynb    bert_train.py             train.py\r\n",
      "Test dataloader.ipynb     bert_train_multi_head.py  \u001b[1m\u001b[34mwiki\u001b[m\u001b[m/\r\n",
      "\u001b[1m\u001b[34m__pycache__\u001b[m\u001b[m/              dataloader.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0112 00:35:30.179898 140734747264448 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/johnhallman/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki data files exist already! loading precomputed values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnhallman/mlcourse/senior_ml/nlp-final-project/src/bert_dataloader.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x = torch.tensor(x)\n",
      "/Users/johnhallman/mlcourse/senior_ml/nlp-final-project/src/bert_dataloader.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.y = torch.tensor(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake news data files exist already! loading precomputed values\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "wiki_data = get_wiki_data(tokenizer)\n",
    "fake_data = get_fake_data(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train, wiki_dev, wiki_test = wiki_data['train'], wiki_data['dev'], wiki_data['test']\n",
    "fake_train, fake_dev, fake_test = fake_data['train'], fake_data['dev'], fake_data['test']\n",
    "wiki_num_labels, fake_num_labels = wiki_train.num_labels(), fake_train.num_labels()\n",
    "wiki_n, fake_n = len(wiki_train), len(fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0112 00:39:45.302918 140734747264448 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/johnhallman/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I0112 00:39:45.304229 140734747264448 configuration_utils.py:199] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": [\n",
      "    2,\n",
      "    4\n",
      "  ],\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0112 00:39:45.387749 140734747264448 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /Users/johnhallman/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0112 00:39:48.378950 140734747264448 modeling_utils.py:483] Weights from pretrained model not used in BertMultiHeadModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "model = BertMultiHeadModel.from_pretrained('bert-base-uncased', num_labels=[2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model accuracy and loss on given dataset\n",
    "def evaluate(model, wiki_data, fake_data, batch_size=32, debugging=False):\n",
    "    t_start = time.time()\n",
    "    model.eval()\n",
    "    wiki_loader = DataLoader(wiki_data, batch_size=batch_size)\n",
    "    fake_loader = DataLoader(fake_data, batch_size=batch_size)\n",
    "    for name, loader, n in zip(['wiki', 'fake'], [wiki_loader, fake_loader], [len(wiki_data), len(fake_data)]):\n",
    "        predictions = np.zeros(n) # used for confusion matrix\n",
    "        truth = np.zeros(n)\n",
    "        total_loss = 0\n",
    "        curr = 0\n",
    "        with torch.no_grad():\n",
    "            for (x, y) in dataloader:\n",
    "                pred = model(x, labels=y)\n",
    "                predictions[curr:min(n,curr+batch_size)] = torch.argmax(pred[1], axis=1)\n",
    "                truth[curr:min(n,curr+batch_size)] = y\n",
    "                total_loss += pred[0].item()\n",
    "                curr += batch_size\n",
    "                if debugging: break # one batch if debugging\n",
    "        mean_loss = total_loss / n\n",
    "        mean_accuracy = np.mean(predictions == truth)\n",
    "        time_taken = time.time() - t_start\n",
    "        print(\"evaluation for \" + name)\n",
    "        print(\"time {}: loss {}, accuracy {}, mean prediction {}\".format(\n",
    "            time_taken, mean_loss, mean_accuracy, np.mean(predictions)))\n",
    "\n",
    "def create_sampler(train):\n",
    "    frequencies = {}\n",
    "    for pair in train: # pair = (x, y)\n",
    "        if pair[1].item() not in frequencies:\n",
    "            frequencies[pair[1].item()] = 0\n",
    "        frequencies[pair[1].item()] += 1\n",
    "    weights = []\n",
    "    for pair in train:\n",
    "        weights.append(1/frequencies[pair[1].item()])\n",
    "    sampler = WeightedRandomSampler(weights=weights, num_samples=len(train))\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0112 10:37:03.231394 140734747264448 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/johnhallman/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I0112 10:37:03.232532 140734747264448 configuration_utils.py:199] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": [\n",
      "    2,\n",
      "    4\n",
      "  ],\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0112 10:37:03.324809 140734747264448 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /Users/johnhallman/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0112 10:37:06.246374 140734747264448 modeling_utils.py:483] Weights from pretrained model not used in BertMultiHeadModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "balance=True\n",
    "\n",
    "# create dataloaders\n",
    "wiki_sampler = create_sampler(wiki_train) if balance else SequentialSampler(wiki_train)\n",
    "wiki_dataloader = DataLoader(wiki_train, sampler=wiki_sampler, batch_size=32)\n",
    "fake_sampler = create_sampler(fake_train) if balance else SequentialSampler(fake_train)\n",
    "fake_dataloader = DataLoader(fake_train, sampler=fake_sampler, batch_size=32)\n",
    "\n",
    "model = BertMultiHeadModel.from_pretrained('bert-base-uncased', num_labels=[2, 4])\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = min(len(wiki_dataloader), len(fake_dataloader)) * 2 # number of batches * number of epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a50884cb7616>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdebugging\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_wiki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_wiki\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fake\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# different shuffle each time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "debugging=False\n",
    "model.train()\n",
    "for epoch in range(1 if debugging else args['epochs']):\n",
    "    for (x_wiki, y_wiki), (x_fake, y_fake) in zip(wiki_dataloader, fake_dataloader): # different shuffle each time\n",
    "        optimizer.zero_grad()\n",
    "        # train on wiki\n",
    "        output = model(0, x_wiki, labels=y_wiki) # 0 => wiki head\n",
    "        loss, preds = output[0], output[1]\n",
    "        total_loss += loss.item()\n",
    "        curr += args['batch_size']\n",
    "        loss.backward() # loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args['clip_grad_norm'])\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # train on fake news\n",
    "        output = model(1, x_fake, labels=y_fake) # 1 => fake news head\n",
    "        loss, preds = output[0], output[1]\n",
    "        total_loss += loss.item()\n",
    "        curr += args['batch_size']\n",
    "        loss.backward() # loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args['clip_grad_norm'])\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if debugging: break # only 1 batch when debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(1 if debugging else args['epochs']):\n",
    "    for (x_batch, y_batch) in train_dataloader: # different shuffle each time\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch, labels=y_batch)\n",
    "        loss, preds = output[0], output[1]\n",
    "        predictions[curr:min(n,curr+args['batch_size'])] = torch.argmax(preds, axis=1)\n",
    "        truth[curr:min(n,curr+args['batch_size'])] = y_batch\n",
    "        total_loss += loss.item()\n",
    "        curr += args['batch_size']\n",
    "        loss.backward() # loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args['clip_grad_norm'])\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
