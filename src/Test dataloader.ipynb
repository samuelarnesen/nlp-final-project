{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, TensorDataset\n",
    "import torch.nn.utils\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's first load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    Class for converting words to indexes and back out again\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, series_list):\n",
    "        \"\"\"\n",
    "        Stores list to map an index number to a word and a dictionary to map a word to an index number\n",
    "\n",
    "        @param series_list (list(pd.Series)): list of pandas series to be used to form vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        vocab_dict = dict()\n",
    "        def add_entry_if_necessary(entry):\n",
    "            words = entry.split()\n",
    "            for word in words:\n",
    "                if word.lower() not in vocab_dict:\n",
    "                    vocab_dict[word.lower()] = True\n",
    "\n",
    "        for series in series_list:\n",
    "            series.apply(add_entry_if_necessary)\n",
    "\n",
    "        self.index_to_word = list(vocab_dict.keys())\n",
    "        self.index_to_word.insert(0, \"<pad>\")\n",
    "        self.index_to_word.insert(1, \"<unk>\")\n",
    "        self.index_to_word.insert(2, \"<s>\")\n",
    "        self.index_to_word.insert(3, \"</s>\")\n",
    "\n",
    "        self.word_to_index = dict()\n",
    "        for idx in range(len(self.index_to_word)):\n",
    "            self.word_to_index[self.index_to_word[idx]] = idx\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"\n",
    "        Allows Vocabulary object to be subscripted\n",
    "\n",
    "        @param item (int or string): if int, gets the word found at that index\n",
    "                                     if string, gets the index associated with that word\n",
    "\n",
    "        \"\"\"\n",
    "        if type(item) == type(\"string\"):\n",
    "            return self.word_to_index[item]\n",
    "        else:\n",
    "            return self.index_to_word[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_to_word)\n",
    "\n",
    "    def get_index_list_from_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Converts a sentence into a list of indices\n",
    "\n",
    "        @param sentence (str): a string of words that is to be converted\n",
    "\n",
    "        @returns idx_list (List(int)): a list of integers where each element corresponds to the index of a word in the input sentence\n",
    "        \"\"\"\n",
    "        idx_list = list()\n",
    "        idx_list.append(self[\"<s>\"])\n",
    "        for word in sentence.split():\n",
    "            if word.lower() not in self.word_to_index:\n",
    "                idx_list.append(self[\"<unk>\"])\n",
    "            else:\n",
    "                idx_list.append(self[word.lower()])\n",
    "        idx_list.append(self[\"</s>\"])\n",
    "        return idx_list\n",
    "\n",
    "    def get_tensor_from_sentences(self, sentences, device: torch.device):\n",
    "        \"\"\"\n",
    "        Makes a torch tensor from a batch of sentences\n",
    "\n",
    "        @param sentences (List(List(int))): the sentences that will comprise the tensor\n",
    "        @param device (torch.device): device code is being run on \n",
    "\n",
    "        @returns tensor (torch.tensor): padded tensor of input sentences \n",
    "        \"\"\"\n",
    "        return torch.t(torch.tensor(self.pad_sentences(sentences), dtype=torch.long, device=device))\n",
    "    \n",
    "    def pad_sentences(self, sentences):\n",
    "        max_length = max(len(sentence) for sentence in sentences)\n",
    "        word_idxs = np.zeros((len(sentences), max_length), dtype=np.dtype(int)) # pad id == 0\n",
    "        for i, s in enumerate(sentences):\n",
    "            word_idxs[i,:len(s)] = s\n",
    "        return word_idxs\n",
    "\n",
    "\n",
    "class WikiDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class for storing input data from Wikipedia dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, comment_df: pd.DataFrame, annotation_df: pd.DataFrame, vocab: Vocabulary):\n",
    "        \"\"\"\n",
    "        @param comment_df (pd.DataFrame): pandas DataFrame with \"comments\" section that is used as the input\n",
    "        @param annotation_df (pd.DataFrame): pandas DataFrame that stores the labels\n",
    "        @param vocab (Vocabulary): vocabulary to be used \n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        cleaned_comment_df = comment_df.copy()\n",
    "        cleaned_comment_df[\"comment\"] = cleaned_comment_df[\"comment\"].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "        cleaned_comment_df[\"comment\"] = cleaned_comment_df[\"comment\"].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "\n",
    "        self.x = cleaned_comment_df[\"comment\"].apply(self.vocab.get_index_list_from_sentence).values\n",
    "        self.x = self.vocab.pad_sentences(self.x)\n",
    "        self.y = (annotation_df[annotation_df[\"rev_id\"].isin(cleaned_comment_df[\"rev_id\"])].groupby(\"rev_id\")[\"attack\"].mean() > 0.5).values\n",
    "        self.y = np.array([int(i) for i in self.y])\n",
    "        self._num_labels = np.max(self.y) + 1\n",
    "        \n",
    "    def num_labels(self):\n",
    "        return self._num_labels\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        @returns (tuple(List(int), bool)): first term is a list of the indices of the words of the input sentence at the specified index\n",
    "                                           second term is boolean corresponding to whether it is an attack (True) or not (False)\n",
    "        \"\"\"\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "\n",
    "class FakeNewsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, body_df: pd.DataFrame, stance_df: pd.DataFrame, vocab: Vocabulary):\n",
    "\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "\n",
    "        stance_to_idx = {}\n",
    "        stances = stance_df[\"Stance\"].drop_duplicates().values\n",
    "        for i, stance in enumerate(stances):\n",
    "            stance_to_idx[stance] = i\n",
    "        num_stances = len(stance_to_idx)\n",
    "\n",
    "        body_df[\"sentence_as_idx\"] = body_df[\"articleBody\"].apply(self.vocab.get_index_list_from_sentence)\n",
    "\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        idx_to_id = {body_id:i for (i, body_id) in enumerate(body_df['Body ID'])}\n",
    "\n",
    "        for body_id, headline, stance in zip(stance_df[\"Body ID\"], stance_df[\"Headline\"], stance_df[\"Stance\"]):\n",
    "            head = vocab.get_index_list_from_sentence(headline)\n",
    "            body = body_df.iloc[idx_to_id[body_id]][\"sentence_as_idx\"]\n",
    "            x_list.append(head + body)\n",
    "            y_list.append(stance_to_idx[stance])\n",
    "\n",
    "        self.x = self.vocab.pad_sentences(x_list)\n",
    "        self.y = np.array(y_list)\n",
    "        self._num_labels = np.max(self.y) + 1\n",
    "\n",
    "    def num_labels(self):\n",
    "        return self._num_labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.read_csv(\"../data/attack_annotated_comments.tsv\", sep ='\\t')\n",
    "body_df = pd.read_csv(\"../data/fake_news_bodies.csv\")\n",
    "stance_df = pd.read_csv(\"../data/fake_news_stances.csv\")\n",
    "annotation_df = pd.read_csv(\"../data/attack_annotations.tsv\",  sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary([comment_df[\"comment\"], body_df[\"articleBody\"], stance_df[\"Headline\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  28.7069571018219\n"
     ]
    }
   ],
   "source": [
    "# takes 426.58s with equal length torch tensors and 27.5s with equal length numpy arrays!\n",
    "t_start = time.time()\n",
    "wiki_dataset = WikiDataset(comment_df, annotation_df, vocab)\n",
    "fake_news_dataset = FakeNewsDataset(body_df, stance_df, vocab)\n",
    "print(\"time: \", time.time() - t_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# looking good so far! let's import some models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" models \"\"\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "\n",
    "\"\"\" Credit: this code was inspired by pytorch.org/tutorials/beginner/transformer_tutorial.html \"\"\"\n",
    "\n",
    "# helper module for our classifier\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        \"\"\" d_model = word embedding dimension of the transformer inputs \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) # add batch dimension\n",
    "        self.register_buffer('pe', pe) # do not perform gradient descent on positional embeddings!!\n",
    "\n",
    "    def forward(self, x): # BERT also adds positional encodings directly to embeddings\n",
    "        x = x + self.pe[:, :x.size(1), :] # only include sentence-length many points\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# implement classification transformer specific for our application\n",
    "class TransformerClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, labels, embedding_dim, nhead, feedforward_dim, nlayers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: number of words/max index of our vocabulary\n",
    "            labels: number of labels in our predictions\n",
    "            embedding_dim: word embedding dimension\n",
    "            nhead: number of attention heads\n",
    "            feedforward_dim: dimension of feedforward layers\n",
    "            nlayers: number of attention layers to stack\n",
    "            dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.labels = labels\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) # word embedding layer\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, dropout) # positional embedding\n",
    "        encoder_layers = TransformerEncoderLayer(embedding_dim, nhead, feedforward_dim, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers) # transformer\n",
    "        self.linear_classifier = nn.Linear(embedding_dim, labels) # transformer output to class scores\n",
    "        self.softmax = nn.Softmax(dim=1) # softmax over scores\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1 # small variance has been shown to lead to better embedding initialization\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear_classifier.bias.data.zero_()\n",
    "        initrange = np.sqrt(6) / np.sqrt(self.embedding_dim + self.labels) # glorot initialization\n",
    "        self.linear_classifier.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\" src must be formatted with dims (batch_size, sentence_length, one_hot_labels) \"\"\"\n",
    "        word_embedding = self.embedding(src)\n",
    "        word_pos_embed = self.pos_encoder(word_embedding)\n",
    "        encoder_output = (self.transformer_encoder(word_pos_embed)[:,0,:]).squeeze(1) # use only the first word's embedding\n",
    "        scores = self.linear_classifier(encoder_output)\n",
    "        softmax_scores = self.softmax(scores) # softmax over scores\n",
    "        return softmax_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ... and now train them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model with provided arguments\n",
    "def model_from_dataset(vocab_size, num_labels, args):\n",
    "    model = TransformerClassifier(vocab_size, num_labels, args['embedding_dim'], args['nheads'], \\\n",
    "        args['layers'], args['feedforward_dim'])\n",
    "    return model\n",
    "\n",
    "# evaluate model accuracy and loss on given dataset\n",
    "def evaluate(model, dataset, num_labels, batch_size=32):\n",
    "    n = len(dataset)\n",
    "    predictions = np.zeros(n) # used for confusion matrix\n",
    "    truth = np.zeros(n) \n",
    "    total_loss = 0\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    curr = 0\n",
    "    with torch.no_grad():\n",
    "        for (x, y) in dataloader:\n",
    "            print(curr)\n",
    "            pred = model(x)\n",
    "            predictions[curr:min(n,curr+batch_size)] = torch.argmax(pred, axis=1)\n",
    "            truth[curr:min(n,curr+batch_size)] = y\n",
    "            total_loss += criterion(pred.view(-1, num_labels), y).item()\n",
    "            curr += batch_size\n",
    "    mean_loss = total_loss / n\n",
    "    mean_accuracy = np.mean(predictions == truth)\n",
    "    return mean_loss, mean_accuracy, predictions, truth\n",
    "\n",
    "# split dataset into train, validation, and test\n",
    "def split_dataset(dataset, train_size, val_size, test_size):\n",
    "    return torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# create/empty directory to save models in\n",
    "def new_dir(dir_path):\n",
    "    if os.path.isdir(dir_path): \n",
    "        print(\"deleting existing directory {}\".format(dir_path))\n",
    "        shutil.rmtree(dir_path)\n",
    "    os.makedirs(dir_path) # check valid directory\n",
    "\n",
    "# method for training transformer model on given dataset\n",
    "def train(vocab_size, num_labels, train_dataset, val_dataset, save_dir, args):\n",
    "    new_dir(save_dir)\n",
    "    \n",
    "    \"\"\" create model and prepare optimizer \"\"\"\n",
    "    model = model_from_dataset(vocab_size, num_labels, args)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args['lr'])\n",
    "    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95) # note: 0.95^13 is approx 0.5\n",
    "    model.train() # turn on train mode\n",
    "    \n",
    "    \"\"\" train model \"\"\"\n",
    "    print(\"starting training\")\n",
    "    for epoch in range(args['epochs']):\n",
    "        total_loss = 0.\n",
    "        start_time = time.time()\n",
    "        for (x_batch, y_batch) in train_dataloader: # different shuffle each time\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = criterion(output.view(-1, num_labels), y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args['clip_grad_norm'])\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # print info from this training epoch\n",
    "        print('| epoch {:3d} | {:5d} samples | '\n",
    "              'lr {:02.2f} | time {:5.2f} | loss {:5.2f}'.format(epoch, len(train_dataset), \n",
    "                args['lr'], elapsed, total_loss)) # or scheduler.get_lr()[0]\n",
    "        \n",
    "        # save model to new directory\n",
    "        if (epoch+1) % args['save_frequency'] == 0 and epoch+1 != args['epochs']:\n",
    "            path = os.path.join(save_dir, \"epoch-{}\".format(epoch))\n",
    "            new_dir(path)\n",
    "            torch.save(model.state_dict(), path) # store trained model\n",
    "            \n",
    "    \"\"\" save final model \"\"\"\n",
    "    path = os.path.join(save_dir, \"final\")\n",
    "    new_dir(path)\n",
    "    torch.save(model.state_dict(), path) # store trained model\n",
    "    \"\"\" \n",
    "    Note: see https://stackoverflow.com/questions/42703500/\n",
    "    best-way-to-save-a-trained-model-in-pytorch — the best way to save a model\n",
    "    is to save the state, then to load using\n",
    "    new_model = TheModelClass(*args, **kwargs)\n",
    "    new_model.load_state_dict(torch.load(path))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = { # very small values for testing purposes\n",
    "    'epochs': 2,\n",
    "    'batch_size': 16,\n",
    "    'embedding_dim': 32,\n",
    "    'nheads': 1,\n",
    "    'layers': 1,\n",
    "    'feedforward_dim': 32,\n",
    "    'lr': 0.01,\n",
    "    'clip_grad_norm': 0.5,\n",
    "    'save_frequency': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "base_dir = os.getcwd()\n",
    "wiki_dir = os.path.join(base_dir, \"wiki\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(vocab)\n",
    "wiki_num_labels = wiki_dataset.num_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(wiki_dataset)\n",
    "#n_train, n_val, n_test = int(0.7*n), int(0.15*n), n - (int(0.7*n) + int(0.15*n))\n",
    "n_train, n_val, n_test = 50, int(0.15*n), n - (50 + int(0.15*n))\n",
    "wiki_train, wiki_val, wiki_test = split_dataset(wiki_dataset, n_train, n_val, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting existing directory /Users/johnhallman/mlcourse/senior_ml/nlp-final-project/src/wiki\n",
      "starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |    50 samples | lr 0.01 | time 17.31 | loss  2.52\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/johnhallman/mlcourse/mlenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-152-e5c120d39415>\", line 1, in <module>\n",
      "    train(vocab_length, wiki_num_labels, wiki_train, wiki_val, wiki_dir, args) # no validation dataset\n",
      "  File \"<ipython-input-150-eab85c1350ea>\", line 76, in train\n",
      "    torch.save(model.state_dict(), path) # store trained model\n",
      "  File \"/Users/johnhallman/mlcourse/mlenv/lib/python3.6/site-packages/torch/serialization.py\", line 260, in save\n",
      "    return _with_file_like(f, \"wb\", lambda f: _save(obj, f, pickle_module, pickle_protocol))\n",
      "  File \"/Users/johnhallman/mlcourse/mlenv/lib/python3.6/site-packages/torch/serialization.py\", line 183, in _with_file_like\n",
      "    f = open(f, mode)\n",
      "IsADirectoryError: [Errno 21] Is a directory: '/Users/johnhallman/mlcourse/senior_ml/nlp-final-project/src/wiki/epoch-0'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/johnhallman/mlcourse/mlenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'IsADirectoryError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/johnhallman/mlcourse/mlenv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/johnhallman/mlcourse/mlenv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/johnhallman/mlcourse/mlenv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\", line 725, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/Users/johnhallman/mlcourse/mlenv/bin/../lib/python3.6/posixpath.py\", line 374, in abspath\n",
      "    cwd = os.getcwd()\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/Users/johnhallman/mlcourse/senior_ml/nlp-final-project/src/wiki/epoch-0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "train(vocab_length, wiki_num_labels, wiki_train, wiki_val, wiki_dir, args) # no validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
