{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataloader import get_data\n",
    "from models import TransformerClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug Models.ipynb     dataloader.py          \u001b[1m\u001b[34mtrained_fake\u001b[m\u001b[m/\r\n",
      "Test dataloader.ipynb  models.py              \u001b[1m\u001b[34mtrained_wiki\u001b[m\u001b[m/\r\n",
      "\u001b[1m\u001b[34m__pycache__\u001b[m\u001b[m/           train.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, data_dict = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data, fake_data = data_dict['wiki'], data_dict['fake news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(nn.Module):\n",
    "    def __init__(self, vocab, num_labels, train_data, alpha=0.001):\n",
    "        super(NaiveBayes, self).__init__()\n",
    "        self.vocab_len = len(vocab)\n",
    "        self.classes = num_labels\n",
    "        self.p_class = np.zeros(self.classes)\n",
    "        self.p_vocab = alpha * np.ones((self.classes, self.vocab_len))\n",
    "        for (x, y) in train_data:\n",
    "            self.p_class[y] += 1\n",
    "            for i in x:\n",
    "                if (i == 0): break # 0 padding\n",
    "                self.p_vocab[y, i] += 1\n",
    "        self.p_class /= np.sum(self.p_class)\n",
    "        self.p_vocab = (self.p_vocab.T / np.sum(self.p_vocab, axis=1)).T\n",
    "        \n",
    "    def forward(self, src):\n",
    "        log_probs = np.log(self.p_class)\n",
    "        for i in src:\n",
    "            if (i == 0): break\n",
    "            log_probs += np.log(self.p_vocab[:,i])\n",
    "        return np.argmax(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = NaiveBayes(vocab, wiki_data.num_labels(), wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model([1, 2, 3 ,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train, validation, and test\n",
    "def split_dataset(dataset, train_size, val_size, test_size):\n",
    "    return torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "def evaluate(model, val_dataset):\n",
    "    n = len(val_dataset)\n",
    "    pred, true = np.zeros(n), np.zeros(n)\n",
    "    for i, (x, y) in enumerate(val_dataset):\n",
    "        true[i] = y\n",
    "        pred[i] = model(x)\n",
    "    print(\"score: \", round(np.mean(pred == true), 4))\n",
    "    return np.mean(pred == true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_on_dataset(model_class, dataset):\n",
    "    n = len(dataset)\n",
    "    n_train, n_val, n_test = n - 2*int(0.15*n), int(0.15*n), int(0.15*n)\n",
    "    train, val, test = split_dataset(dataset, n_train, n_val, n_test)\n",
    "    best_model, best_score = None, 0.0\n",
    "    for alpha in [0.01, 0.05, 0.1, 0.5, 1, 3, 6, 10]:\n",
    "        nb_model = model_class(vocab, dataset.num_labels(), train, alpha=alpha)\n",
    "        print(\"evaluate model, alpha = \", alpha)\n",
    "        score = evaluate(nb_model, val)\n",
    "        if (score > best_score):\n",
    "            best_model = nb_model\n",
    "            best_score = score\n",
    "    print(\"best model, evaluated on final test dataset\")\n",
    "    return evaluate(nb_model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate model, alpha =  0.01\n",
      "score:  0.8575\n",
      "evaluate model, alpha =  0.05\n",
      "score:  0.8578\n",
      "evaluate model, alpha =  0.1\n",
      "score:  0.8699\n",
      "evaluate model, alpha =  0.5\n",
      "score:  0.9224\n",
      "evaluate model, alpha =  1\n",
      "score:  0.917\n",
      "evaluate model, alpha =  3\n",
      "score:  0.8975\n",
      "evaluate model, alpha =  6\n",
      "score:  0.8918\n",
      "evaluate model, alpha =  10\n",
      "score:  0.8893\n",
      "best model, evaluated on final test dataset\n",
      "score:  0.886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.885959245992131"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model_on_dataset(NaiveBayes, wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate model, alpha =  0.01\n",
      "score:  0.5703\n",
      "evaluate model, alpha =  0.05\n",
      "score:  0.5833\n",
      "evaluate model, alpha =  0.1\n",
      "score:  0.5999\n",
      "evaluate model, alpha =  0.5\n",
      "score:  0.723\n",
      "evaluate model, alpha =  1\n",
      "score:  0.7574\n",
      "evaluate model, alpha =  3\n",
      "score:  0.7611\n",
      "evaluate model, alpha =  6\n",
      "score:  0.7404\n",
      "evaluate model, alpha =  10\n",
      "score:  0.7404\n",
      "best model, evaluated on final test dataset\n",
      "score:  0.7283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7282993197278912"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model_on_dataset(NaiveBayes, fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slightly more sophisticated model\n",
    "class NaiveBayesBigram(nn.Module):\n",
    "    def __init__(self, vocab, num_labels, train_data, alpha=0.001):\n",
    "        super(NaiveBayesBigram, self).__init__()\n",
    "        self.vocab_len = len(vocab)\n",
    "        self.classes = num_labels\n",
    "        self.p_class = np.zeros(self.classes)\n",
    "        self.p_vocab = alpha * np.ones((self.classes, self.vocab_len, self.vocab_len))\n",
    "        for (x, y) in train_data:\n",
    "            self.p_class[y] += 1\n",
    "            for i in range(len(x)):\n",
    "                if i == 0: continue # skip first one\n",
    "                if (x[i+1] == 0): break # 0 padding\n",
    "                self.p_vocab[y, x[i], x[i+1]] += 1\n",
    "        self.p_class /= np.sum(self.p_class)\n",
    "        for i in range(self.p_vocab.shape[0]):\n",
    "            self.p_vocab[i] = self.p_vocab[i] / np.sum(self.p_vocab[i])\n",
    "        \n",
    "    def forward(self, src):\n",
    "        log_probs = np.log(self.p_class)\n",
    "        for i in range(len(src)):\n",
    "            if (i == 0): continue\n",
    "            if (src[i+1] == 0): break\n",
    "            log_probs += np.log(self.p_vocab[:, src[i], src[i+1]])\n",
    "        return np.argmax(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model_on_dataset(NaiveBayesBigram, wiki_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cutoff = 0.3\n",
    "comment_df = pd.read_csv(\"../data/attack_annotated_comments.tsv\", sep ='\\t')\n",
    "comment_df[\"comment\"] = comment_df[\"comment\"].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comment_df[\"comment\"] = comment_df[\"comment\"].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "annotation_df = pd.read_csv(\"../data/attack_annotations.tsv\",  sep='\\t')\n",
    "annotation_df = (annotation_df.groupby(\"rev_id\")[\"attack\"].mean() > cutoff)\n",
    "annotation_df = annotation_df.to_frame().reset_index()\n",
    "final_df = pd.merge(comment_df, annotation_df, how='inner', on=['rev_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iraq is not good  ===  ===  USA is bad   \n",
      "\n",
      "Anon  :What the heck are you talking about? This is an encyclopedia, not a book store. \n",
      "\n",
      "i have a dick, its bigger than yours! hahaha\n",
      "\n",
      "I'm uber-anal about my AFI\n",
      "\n",
      "  You are a child.  \n",
      "\n",
      "  i am using the sandbox, ass wipe\n",
      "\n",
      "  == GOD DAMN ==  GOD DAMN it fuckers, i am using the GOD DAMN sand box\n",
      "\n",
      "GOD DAMN GOD DAMN GOD DAMN GOD DAMN GOD DAMN GOD DAMN GOD DAMN GOD DAMN GOD DAMN GOD DAMN\n",
      "\n",
      "  == I HAVE A HARD ON FOR WAR ==  SUP, NIGGS?????????????????????111111111111111111111111\n",
      "\n",
      "  == whoa ==  you are a bi fat idiot, stop spamming my user space.\n",
      "\n",
      "  == whoa ==  you are a big fat idot, stop spamming my userspace\n",
      "\n",
      "  == KASH JAFFREY ==  ADD KASH JAFFREY, DO YOU HAVE BIAS AGAINST CANADIANS?\n",
      "\n",
      "  ::: Hey Bobby, get a clue, I dont deal with tards or liars.  \n",
      "\n",
      "Hey  Kerry is a loser and so is Gore.-   \n",
      "\n",
      "  WOW MIRV IS TEH protect THIS PAEG!!!  LOL, CHARLES/MIRV SEEMS TO BE A REAL FAG.   FUCKING ASSWIPE\n",
      "\n",
      "  Vandals stop messing with this page!\n",
      "\n",
      "  Maybe that is the name his mother gave him!\n",
      "\n",
      "   ==Can I Eye== FUCK YOU YOU NEGRO SEE GROW  \n",
      "\n",
      "   ==Thank You your the Wellcome== ARABIC ASSHOLR GO BACK TO ARABIA YOU FUCKING ARABIC SAUDI!!  \n",
      "\n",
      " :well that's just peachy, now go do something else.\n",
      "\n",
      " I responded to everything; you responded with more empty statements that proved nothing.  \n",
      "\n",
      " Can you read? I attacked you when there was no substance to respond to. You're impossible.  \n",
      "\n",
      "   What the fuck is your problem?\n",
      "\n",
      "And last but not least, yes, he WAS a crook. \n",
      "\n",
      "  Shut the hell up JeremyA stop posting nonsense.\n",
      "\n",
      "  1. punk rock because they band say they are you (cant swear but i hate you).\n",
      "\n",
      ", obviously Lan3y knows more about the band than mark and tom, an  example of Stupidity.\n",
      "\n",
      " KISS MY HAIRY BOTTAY    SCREW YOU ALL IF YOU LIKE DICTATORS DIE DIE DIE  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(final_df[final_df['attack'] == True]['comment']):\n",
    "    if len(row) < 100:\n",
    "        print(row)\n",
    "        print()\n",
    "    if i > 100: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This page will need disambiguation. \n",
      "\n",
      "   On my  you will find the apology that I owe you.  [ shuffles his feet and looks at the floor]\n",
      "\n",
      "gets far more tendentious yet. \n",
      "\n",
      "    Yep, that's Twin cities from which this article originated.\n",
      "\n",
      " See? I was right! ;-)    \n",
      "\n",
      "`    Thanks for fixing the spelling error in ``propaganda``.  I can't believe I did that.  `\n",
      "\n",
      "  :::Yet more:  Follow the whole thread if you like. mav\n",
      "\n",
      "  Dear Mav: Thanks, bro! God bless you!  Sincerely yours, *\n",
      "\n",
      "MarcusAurelius]]   Why exactly was this user banned? \n",
      "\n",
      "Really, is it WIntel, this is, W(indows) + INTEL ??\n",
      "\n",
      "  I just save an awful lot -). - fonzy\n",
      "\n",
      "`  :Excuse me, what ``name`` did I call you? `\n",
      "\n",
      "  20:52 Apr 12, 2003 (UTC)\n",
      "\n",
      "  Oh the joy of nutters! Anyway, I see the Rachel Corrie photomontage shrine is back. -(\n",
      "\n",
      "How is one to fix the problem with the Schopenhauer nested link? \n",
      "\n",
      "  Wikipedia is banned? Why?  \n",
      "\n",
      "    Do you think European Union should have the form of the Wikipedia project countries?  \n",
      "\n",
      "to-do: get name of Dexter's friend and any missing characters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(final_df[final_df['attack'] == False]['comment']):\n",
    "    if len(row) < 100:\n",
    "        print(row)\n",
    "        print()\n",
    "    if i > 100: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
