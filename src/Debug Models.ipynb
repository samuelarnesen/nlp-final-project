{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataloader import get_data\n",
    "from models import TransformerClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug Models.ipynb     dataloader.py          \u001b[1m\u001b[34mtrained_fake\u001b[m\u001b[m/\r\n",
      "Test dataloader.ipynb  models.py              \u001b[1m\u001b[34mtrained_wiki\u001b[m\u001b[m/\r\n",
      "\u001b[1m\u001b[34m__pycache__\u001b[m\u001b[m/           train.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, data_dict = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data, fake_data = data_dict['wiki'], data_dict['fake news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(nn.Module):\n",
    "    def __init__(self, vocab, num_labels, train_data, alpha=0.001):\n",
    "        super(NaiveBayes, self).__init__()\n",
    "        self.vocab_len = len(vocab)\n",
    "        self.classes = num_labels\n",
    "        self.p_class = np.zeros(self.classes)\n",
    "        self.p_vocab = alpha * np.ones((self.classes, self.vocab_len))\n",
    "        for (x, y) in train_data:\n",
    "            self.p_class[y] += 1\n",
    "            for i in x:\n",
    "                if (i == 0): break # 0 padding\n",
    "                self.p_vocab[y, i] += 1\n",
    "        self.p_class /= np.sum(self.p_class)\n",
    "        self.p_vocab = (self.p_vocab.T / np.sum(self.p_vocab, axis=1)).T\n",
    "        \n",
    "    def forward(self, src):\n",
    "        log_probs = np.log(self.p_class)\n",
    "        for i in src:\n",
    "            if (i == 0): break\n",
    "            log_probs += np.log(self.p_vocab[:,i])\n",
    "        return np.argmax(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = NaiveBayes(vocab, wiki_data.num_labels(), wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model([1, 2, 3 ,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train, validation, and test\n",
    "def split_dataset(dataset, train_size, val_size, test_size):\n",
    "    return torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "def evaluate(model, val_dataset):\n",
    "    n = len(val_dataset)\n",
    "    pred, true = np.zeros(n), np.zeros(n)\n",
    "    for i, (x, y) in enumerate(val_dataset):\n",
    "        true[i] = y\n",
    "        pred[i] = model(x)\n",
    "    print(\"score: \", round(np.mean(pred == true), 4))\n",
    "    return np.mean(pred == true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_on_dataset(model_class, dataset):\n",
    "    n = len(dataset)\n",
    "    n_train, n_val, n_test = n - 2*int(0.15*n), int(0.15*n), int(0.15*n)\n",
    "    train, val, test = split_dataset(dataset, n_train, n_val, n_test)\n",
    "    best_model, best_score = None, 0.0\n",
    "    for alpha in [0.01, 0.05, 0.1, 0.5, 1, 3, 6, 10]:\n",
    "        nb_model = model_class(vocab, dataset.num_labels(), train, alpha=alpha)\n",
    "        print(\"evaluate model, alpha = \", alpha)\n",
    "        score = evaluate(nb_model, val)\n",
    "        if (score > best_score):\n",
    "            best_model = nb_model\n",
    "            best_score = score\n",
    "    print(\"best model, evaluated on final test dataset\")\n",
    "    return evaluate(nb_model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate model, alpha =  0.01\n",
      "score:  0.8575\n",
      "evaluate model, alpha =  0.05\n",
      "score:  0.8578\n",
      "evaluate model, alpha =  0.1\n",
      "score:  0.8699\n",
      "evaluate model, alpha =  0.5\n",
      "score:  0.9224\n",
      "evaluate model, alpha =  1\n",
      "score:  0.917\n",
      "evaluate model, alpha =  3\n",
      "score:  0.8975\n",
      "evaluate model, alpha =  6\n",
      "score:  0.8918\n",
      "evaluate model, alpha =  10\n",
      "score:  0.8893\n",
      "best model, evaluated on final test dataset\n",
      "score:  0.886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.885959245992131"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model_on_dataset(NaiveBayes, wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate model, alpha =  0.01\n",
      "score:  0.5703\n",
      "evaluate model, alpha =  0.05\n",
      "score:  0.5833\n",
      "evaluate model, alpha =  0.1\n",
      "score:  0.5999\n",
      "evaluate model, alpha =  0.5\n",
      "score:  0.723\n",
      "evaluate model, alpha =  1\n",
      "score:  0.7574\n",
      "evaluate model, alpha =  3\n",
      "score:  0.7611\n",
      "evaluate model, alpha =  6\n",
      "score:  0.7404\n",
      "evaluate model, alpha =  10\n",
      "score:  0.7404\n",
      "best model, evaluated on final test dataset\n",
      "score:  0.7283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7282993197278912"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model_on_dataset(NaiveBayes, fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slightly more sophisticated model\n",
    "class NaiveBayesBigram(nn.Module):\n",
    "    def __init__(self, vocab, num_labels, train_data, alpha=0.001):\n",
    "        super(NaiveBayesBigram, self).__init__()\n",
    "        self.vocab_len = len(vocab)\n",
    "        self.classes = num_labels\n",
    "        self.p_class = np.zeros(self.classes)\n",
    "        self.p_vocab = alpha * np.ones((self.classes, self.vocab_len, self.vocab_len))\n",
    "        for (x, y) in train_data:\n",
    "            self.p_class[y] += 1\n",
    "            for i in range(len(x)):\n",
    "                if i == 0: continue # skip first one\n",
    "                if (x[i+1] == 0): break # 0 padding\n",
    "                self.p_vocab[y, x[i], x[i+1]] += 1\n",
    "        self.p_class /= np.sum(self.p_class)\n",
    "        for i in range(self.p_vocab.shape[0]):\n",
    "            self.p_vocab[i] = self.p_vocab[i] / np.sum(self.p_vocab[i])\n",
    "        \n",
    "    def forward(self, src):\n",
    "        log_probs = np.log(self.p_class)\n",
    "        for i in range(len(src)):\n",
    "            if (i == 0): continue\n",
    "            if (src[i+1] == 0): break\n",
    "            log_probs += np.log(self.p_vocab[:, src[i], src[i+1]])\n",
    "        return np.argmax(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model_on_dataset(NaiveBayesBigram, wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
