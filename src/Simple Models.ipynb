{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataloader import get_data\n",
    "from models import TransformerClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug Models.ipynb        bert_dataloader.py        models.py\r\n",
      "Evaluate.ipynb            bert_evaluate.py          multi_fake_pickle.p\r\n",
      "Huggingface Bert.ipynb    bert_models.py            multi_wiki_pickle.p\r\n",
      "Model Loading.ipynb       bert_train.py             \u001b[1m\u001b[34mprecomputed\u001b[m\u001b[m/\r\n",
      "Multi Head model.ipynb    bert_train_multi_head.py  train.py\r\n",
      "Test dataloader.ipynb     dataloader.py             \u001b[1m\u001b[34mwiki\u001b[m\u001b[m/\r\n",
      "\u001b[1m\u001b[34m__pycache__\u001b[m\u001b[m/              fake_eval.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, data_dict = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data, fake_data = data_dict['wiki'], data_dict['fake news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(nn.Module):\n",
    "    def __init__(self, vocab, num_labels, train_data, alpha=0.001):\n",
    "        super(NaiveBayes, self).__init__()\n",
    "        self.vocab_len = len(vocab)\n",
    "        self.classes = num_labels\n",
    "        self.p_class = np.zeros(self.classes)\n",
    "        self.p_vocab = alpha * np.ones((self.classes, self.vocab_len))\n",
    "        for (x, y) in train_data:\n",
    "            self.p_class[y] += 1\n",
    "            for i in x:\n",
    "                if (i == 0): break # 0 padding\n",
    "                self.p_vocab[y, i] += 1\n",
    "        self.p_class /= np.sum(self.p_class)\n",
    "        self.p_vocab = (self.p_vocab.T / np.sum(self.p_vocab, axis=1)).T\n",
    "        \n",
    "    def forward(self, src):\n",
    "        log_probs = np.log(self.p_class)\n",
    "        for i in src:\n",
    "            if (i == 0): break\n",
    "            log_probs += np.log(self.p_vocab[:,i])\n",
    "        return np.argmax(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = NaiveBayes(vocab, wiki_data.num_labels(), wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model([1, 2, 3 ,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train, validation, and test\n",
    "def split_dataset(dataset, train_size, val_size, test_size):\n",
    "    return torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "def evaluate(model, val_dataset):\n",
    "    n = len(val_dataset)\n",
    "    pred, true = np.zeros(n, dtype=int), np.zeros(n, dtype=int)\n",
    "    for i, (x, y) in enumerate(val_dataset):\n",
    "        true[i] = y\n",
    "        pred[i] = model(x)\n",
    "    print(\"accuracy: \", round(np.mean(pred == true), 4))\n",
    "    class_scores = [np.mean(pred[true==i] == true[true==i]) for i in range(np.max(true)+1)]\n",
    "    print(\"class-wise accuracies: \", class_scores)\n",
    "    return np.mean(pred == true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_on_dataset(model_class, dataset):\n",
    "    n = len(dataset)\n",
    "    n_train, n_val, n_test = n - 2*int(0.15*n), int(0.15*n), int(0.15*n)\n",
    "    train, val, test = split_dataset(dataset, n_train, n_val, n_test)\n",
    "    best_model, best_score = None, 0.0\n",
    "    for alpha in [0.01, 0.05, 0.1, 0.5, 1, 3, 6, 10]:\n",
    "        nb_model = model_class(vocab, dataset.num_labels(), train, alpha=alpha)\n",
    "        print(\"evaluate model, alpha = \", alpha)\n",
    "        score = evaluate(nb_model, val)\n",
    "        if (score > best_score):\n",
    "            best_model = nb_model\n",
    "            best_score = score\n",
    "    print(\"best model, evaluated on final test dataset\")\n",
    "    return evaluate(nb_model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate model, alpha =  0.01\n",
      "accuracy:  0.8514\n",
      "class-wise accuracies:  [0.8589156546552525, 0.7933673469387755]\n",
      "evaluate model, alpha =  0.05\n",
      "accuracy:  0.8547\n",
      "class-wise accuracies:  [0.8591147388678745, 0.8204081632653061]\n",
      "evaluate model, alpha =  0.1\n",
      "accuracy:  0.8683\n",
      "class-wise accuracies:  [0.8769659565996416, 0.801530612244898]\n",
      "evaluate model, alpha =  0.5\n",
      "accuracy:  0.9218\n",
      "class-wise accuracies:  [0.9688765014267702, 0.560204081632653]\n",
      "evaluate model, alpha =  1\n",
      "accuracy:  0.9172\n",
      "class-wise accuracies:  [0.9913730174530493, 0.3469387755102041]\n",
      "evaluate model, alpha =  3\n",
      "accuracy:  0.8966\n",
      "class-wise accuracies:  [0.9993363859579268, 0.10714285714285714]\n",
      "evaluate model, alpha =  6\n",
      "accuracy:  0.8889\n",
      "class-wise accuracies:  [0.9998009157873781, 0.036224489795918365]\n",
      "evaluate model, alpha =  10\n",
      "accuracy:  0.8873\n",
      "class-wise accuracies:  [1.0, 0.020918367346938777]\n",
      "best model, evaluated on final test dataset\n",
      "accuracy:  0.8837\n",
      "class-wise accuracies:  [0.999933368869936, 0.02028698664027709]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8836690351752892"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model_on_dataset(NaiveBayes, wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate model, alpha =  0.01\n",
      "accuracy:  0.5635\n",
      "class-wise accuracies:  [0.519542772861357, 0.540952380952381, 0.6992481203007519, 0.7468454258675079]\n",
      "evaluate model, alpha =  0.05\n",
      "accuracy:  0.5796\n",
      "class-wise accuracies:  [0.5444321533923304, 0.5219047619047619, 0.631578947368421, 0.748422712933754]\n",
      "evaluate model, alpha =  0.1\n",
      "accuracy:  0.6095\n",
      "class-wise accuracies:  [0.5877581120943953, 0.49714285714285716, 0.6015037593984962, 0.75]\n",
      "evaluate model, alpha =  0.5\n",
      "accuracy:  0.7216\n",
      "class-wise accuracies:  [0.7771017699115044, 0.3314285714285714, 0.09774436090225563, 0.7113564668769716]\n",
      "evaluate model, alpha =  1\n",
      "accuracy:  0.7611\n",
      "class-wise accuracies:  [0.8724188790560472, 0.14476190476190476, 0.0, 0.6198738170347003]\n",
      "evaluate model, alpha =  3\n",
      "accuracy:  0.7615\n",
      "class-wise accuracies:  [0.9917035398230089, 0.005714285714285714, 0.0, 0.1695583596214511]\n",
      "evaluate model, alpha =  6\n",
      "accuracy:  0.7384\n",
      "class-wise accuracies:  [1.0, 0.005714285714285714, 0.0, 0.0]\n",
      "evaluate model, alpha =  10\n",
      "accuracy:  0.7384\n",
      "class-wise accuracies:  [1.0, 0.005714285714285714, 0.0, 0.0]\n",
      "best model, evaluated on final test dataset\n",
      "accuracy:  0.7336\n",
      "class-wise accuracies:  [1.0, 0.001876172607879925, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7336054421768707"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model_on_dataset(NaiveBayes, fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slightly more sophisticated model\n",
    "class NaiveBayesBigram(nn.Module):\n",
    "    def __init__(self, vocab, num_labels, train_data, alpha=0.001):\n",
    "        super(NaiveBayesBigram, self).__init__()\n",
    "        self.vocab_len = len(vocab)\n",
    "        self.classes = num_labels\n",
    "        self.p_class = np.zeros(self.classes)\n",
    "        self.p_vocab = {y:{} for y in np.arange(self.classes)} # dict of dicts: second key is bigram tuple\n",
    "        self.class_sum = np.zeros(self.classes) # NOT THE SAME AS P_CLASS\n",
    "        for (x, y) in train_data:\n",
    "            self.p_class[y] += 1\n",
    "            for i in range(len(x)):\n",
    "                if i+1 == len(x) or x[i+1] == 0: break # 0 padding\n",
    "                if (x[i], x[i+1]) not in self.p_vocab[y]: \n",
    "                    self.p_vocab[y][(x[i], x[i+1])] = 0\n",
    "                self.p_vocab[y][(x[i], x[i+1])] += 1\n",
    "                self.class_sum[y] += 1\n",
    "        for y, d in self.p_vocab.items():\n",
    "            for bigram in d.keys():\n",
    "                d[bigram] = (d[bigram] + alpha) / (self.class_sum[y] + self.vocab_len**2 * alpha)\n",
    "        self.p_class /= np.sum(self.p_class)\n",
    "        self.no_show = alpha * np.ones(self.classes) / (self.class_sum + self.vocab_len**2 * alpha)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        log_probs = np.log(self.p_class)\n",
    "        for i in range(len(src)):\n",
    "            if i+1 == len(src) or src[i+1] == 0: break # 0 padding\n",
    "            bigram = (src[i], src[i+1])\n",
    "            scores = [self.p_vocab[y][bigram] if bigram in self.p_vocab[y] else self.no_show[y] \\\n",
    "                      for y in range(self.classes)]\n",
    "            log_probs += np.log(scores)\n",
    "        return np.argmax(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate model, alpha =  0.01\n",
      "accuracy:  0.9088\n",
      "class-wise accuracies:  [0.9977532544769708, 0.19883966244725737]\n",
      "evaluate model, alpha =  0.05\n",
      "accuracy:  0.9062\n",
      "class-wise accuracies:  [0.9986783849864534, 0.16772151898734178]\n",
      "evaluate model, alpha =  0.1\n",
      "accuracy:  0.9045\n",
      "class-wise accuracies:  [0.9988105464878081, 0.1518987341772152]\n",
      "evaluate model, alpha =  0.5\n",
      "accuracy:  0.9008\n",
      "class-wise accuracies:  [0.9996695962466133, 0.11128691983122363]\n",
      "evaluate model, alpha =  1\n",
      "accuracy:  0.8993\n",
      "class-wise accuracies:  [0.9998678384986454, 0.09651898734177215]\n",
      "evaluate model, alpha =  3\n",
      "accuracy:  0.8968\n",
      "class-wise accuracies:  [1.0, 0.07278481012658228]\n",
      "evaluate model, alpha =  6\n",
      "accuracy:  0.8952\n",
      "class-wise accuracies:  [1.0, 0.058544303797468354]\n",
      "evaluate model, alpha =  10\n",
      "accuracy:  0.8943\n",
      "class-wise accuracies:  [1.0, 0.05063291139240506]\n",
      "best model, evaluated on final test dataset\n",
      "accuracy:  0.8948\n",
      "class-wise accuracies:  [1.0, 0.07057602490918526]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8948264724881085"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model_on_dataset(NaiveBayesBigram, wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate model, alpha =  0.01\n",
      "accuracy:  0.8018\n",
      "class-wise accuracies:  [0.9987049028677151, 0.1297709923664122, 0.015384615384615385, 0.3292021688613478]\n",
      "evaluate model, alpha =  0.05\n",
      "accuracy:  0.7985\n",
      "class-wise accuracies:  [0.9985198889916743, 0.11068702290076336, 0.015384615384615385, 0.31913245546088304]\n",
      "evaluate model, alpha =  0.1\n",
      "accuracy:  0.7965\n",
      "class-wise accuracies:  [0.9987049028677151, 0.09541984732824428, 0.007692307692307693, 0.313710302091402]\n",
      "evaluate model, alpha =  0.5\n",
      "accuracy:  0.7854\n",
      "class-wise accuracies:  [0.9994449583718779, 0.04198473282442748, 0.0, 0.2703330751355538]\n",
      "evaluate model, alpha =  1\n",
      "accuracy:  0.7739\n",
      "class-wise accuracies:  [0.9994449583718779, 0.02099236641221374, 0.0, 0.21301316808675447]\n",
      "evaluate model, alpha =  3\n",
      "accuracy:  0.7544\n",
      "class-wise accuracies:  [0.9994449583718779, 0.0019083969465648854, 0.0, 0.10999225406661503]\n",
      "evaluate model, alpha =  6\n",
      "accuracy:  0.7464\n",
      "class-wise accuracies:  [0.9994449583718779, 0.0019083969465648854, 0.0, 0.06429124709527498]\n",
      "evaluate model, alpha =  10\n",
      "accuracy:  0.741\n",
      "class-wise accuracies:  [1.0, 0.0019083969465648854, 0.0, 0.030983733539891558]\n",
      "best model, evaluated on final test dataset\n",
      "accuracy:  0.7331\n",
      "class-wise accuracies:  [1.0, 0.009328358208955223, 0.0, 0.03023598820058997]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7330612244897959"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model_on_dataset(NaiveBayesBigram, fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD NAIVE BAYES - VERY BAD, DONT USE (runs out of memory)\n",
    "\"\"\"\n",
    "# slightly more sophisticated model\n",
    "class NaiveBayesBigram(nn.Module):\n",
    "    def __init__(self, vocab, num_labels, train_data, alpha=0.001):\n",
    "        super(NaiveBayesBigram, self).__init__()\n",
    "        self.vocab_len = len(vocab)\n",
    "        self.classes = num_labels\n",
    "        self.p_class = np.zeros(self.classes)\n",
    "        self.p_vocab = alpha * np.ones((self.classes, self.vocab_len, self.vocab_len))\n",
    "        print(\"start training\")\n",
    "        for (x, y) in train_data:\n",
    "            self.p_class[y] += 1\n",
    "            for i in range(len(x)):\n",
    "                if i == 0: continue # skip first one\n",
    "                if (x[i+1] == 0): break # 0 padding\n",
    "                self.p_vocab[y, x[i], x[i+1]] += 1\n",
    "        print(\"done with training\")\n",
    "        self.p_class /= np.sum(self.p_class)\n",
    "        for i in range(self.p_vocab.shape[0]):\n",
    "            self.p_vocab[i] = self.p_vocab[i] / np.sum(self.p_vocab[i])\n",
    "        \n",
    "    def forward(self, src):\n",
    "        log_probs = np.log(self.p_class)\n",
    "        for i in range(len(src)):\n",
    "            if (i == 0): continue\n",
    "            if (src[i+1] == 0): break\n",
    "            log_probs += np.log(self.p_vocab[:, src[i], src[i+1]])\n",
    "        return np.argmax(log_probs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# wiki dataset\n",
    "cutoff = 0.3\n",
    "comment_df = pd.read_csv(\"../data/attack_annotated_comments.tsv\", sep ='\\t')\n",
    "comment_df[\"comment\"] = comment_df[\"comment\"].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comment_df[\"comment\"] = comment_df[\"comment\"].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "annotation_df = pd.read_csv(\"../data/attack_annotations.tsv\",  sep='\\t')\n",
    "annotation_df = (annotation_df.groupby(\"rev_id\")[\"attack\"].mean() > cutoff)\n",
    "annotation_df = annotation_df.to_frame().reset_index()\n",
    "final_df = pd.merge(comment_df, annotation_df, how='inner', on=['rev_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iraq is not good  ===  ===  USA is bad   \n",
      "\n",
      "Anon  :What the heck are you talking about? This is an encyclopedia, not a book store. \n",
      "\n",
      "i have a dick, its bigger than yours! hahaha\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(final_df[final_df['attack'] == True]['comment']):\n",
    "    if len(row) < 100:\n",
    "        print(row)\n",
    "        print()\n",
    "    if i > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This page will need disambiguation. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(final_df[final_df['attack'] == False]['comment']):\n",
    "    if len(row) < 100:\n",
    "        print(row)\n",
    "        print()\n",
    "    if i > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake news dataset\n",
    "body_df = pd.read_csv(\"../data/fake_news_bodies.csv\")\n",
    "stance_df = pd.read_csv(\"../data/fake_news_stances.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972\n",
      "115864\n"
     ]
    }
   ],
   "source": [
    "print(len(stance_df))\n",
    "print(len(comment_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
