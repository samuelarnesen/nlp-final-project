{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataloader import get_data\n",
    "from models import TransformerClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate.ipynb            \u001b[1m\u001b[34m__pycache__\u001b[m\u001b[m/              dataloader.py\r\n",
      "Huggingface Bert.ipynb    bert_dataloader.py        models.py\r\n",
      "Model Loading.ipynb       bert_evaluate.py          \u001b[1m\u001b[34mprecomputed\u001b[m\u001b[m/\r\n",
      "Multi Head model.ipynb    bert_models.py            train.py\r\n",
      "Simple Models.ipynb       bert_train.py\r\n",
      "Test dataloader.ipynb     bert_train_multi_head.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, data_dict = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data, fake_data = data_dict['wiki'], data_dict['fake news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(nn.Module):\n",
    "    def __init__(self, vocab, num_labels, train_data, alpha=0.001):\n",
    "        super(NaiveBayes, self).__init__()\n",
    "        self.vocab_len = len(vocab)\n",
    "        self.classes = num_labels\n",
    "        self.p_class = np.zeros(self.classes)\n",
    "        self.p_vocab = alpha * np.ones((self.classes, self.vocab_len))\n",
    "        for (x, y) in train_data:\n",
    "            self.p_class[y] += 1\n",
    "            for i in x:\n",
    "                if (i == 0): break # 0 padding\n",
    "                self.p_vocab[y, i] += 1\n",
    "        self.p_class /= np.sum(self.p_class)\n",
    "        self.p_vocab = (self.p_vocab.T / np.sum(self.p_vocab, axis=1)).T\n",
    "        \n",
    "    def forward(self, src):\n",
    "        log_probs = np.log(self.p_class)\n",
    "        for i in src:\n",
    "            if (i == 0): break\n",
    "            log_probs += np.log(self.p_vocab[:,i])\n",
    "        return np.argmax(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = NaiveBayes(vocab, wiki_data.num_labels(), wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model([1, 2, 3 ,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train, validation, and test\n",
    "def split_dataset(dataset, train_size, val_size, test_size):\n",
    "    return torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "def evaluate(model, val_dataset):\n",
    "    n = len(val_dataset)\n",
    "    pred, true = np.zeros(n, dtype=int), np.zeros(n, dtype=int)\n",
    "    for i, (x, y) in enumerate(val_dataset):\n",
    "        true[i] = y\n",
    "        pred[i] = model(x)\n",
    "    print(\"accuracy: \", round(np.mean(pred == true), 4))\n",
    "    class_scores = [np.mean(pred[true==i] == true[true==i]) for i in range(np.max(true)+1)]\n",
    "    print(\"class-wise accuracies: \", class_scores)\n",
    "    return np.mean(pred == true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_on_dataset(model_class, dataset):\n",
    "    n = len(dataset)\n",
    "    n_train, n_val, n_test = n - 2*int(0.2*n), int(0.2*n), int(0.2*n)\n",
    "    train, val, test = split_dataset(dataset, n_train, n_val, n_test)\n",
    "    best_model, best_score, best_alpha = None, 0.0, 0.0\n",
    "    for alpha in [0.01, 0.05, 0.1, 0.5, 1, 3, 6, 10]:\n",
    "        nb_model = model_class(vocab, dataset.num_labels(), train, alpha=alpha)\n",
    "        print(\"evaluate model, alpha = \", alpha)\n",
    "        score = evaluate(nb_model, val)\n",
    "        if (score > best_score):\n",
    "            best_model = nb_model\n",
    "            best_score = score\n",
    "            best_alpha = alpha\n",
    "    print(\"best model alpha={}, evaluating on final test dataset\".format(best_alpha))\n",
    "    return evaluate(nb_model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_f1_scores(model_class, dataset, alpha, multi_label=False):\n",
    "    labels = ['unrelated', 'agree', 'disagree', 'discuss'] if multi_label else ['not attack', 'attack']\n",
    "    n = len(dataset)\n",
    "    n_train, n_val, n_test = n - 2*int(0.15*n), int(0.15*n), int(0.15*n)\n",
    "    train, val, test = split_dataset(dataset, n_train, n_val, n_test)\n",
    "    nb_model = model_class(vocab, dataset.num_labels(), train, alpha=alpha)\n",
    "    preds, truth = np.zeros(n, dtype=int), np.zeros(n, dtype=int)\n",
    "    for i, (x, y) in enumerate(test):\n",
    "        truth[i] = y\n",
    "        preds[i] = nb_model(x)\n",
    "    for i, label in enumerate(labels): # 4 classes\n",
    "        print(\"label '{}':\".format(label))\n",
    "        precision = np.mean(preds[preds==i] == truth[preds==i])\n",
    "        recall = np.mean(preds[truth==i] == truth[truth==i])\n",
    "        f1_score = 2 * precision * recall / (precision + recall)\n",
    "        print(\"f1 score: {:5f}, precision: {:5}, recall: {:5f}\".format(f1_score, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate model, alpha =  0.01\n",
      "accuracy:  0.8559\n",
      "class-wise accuracies:  [0.8636910129245566, 0.7988338192419825]\n",
      "evaluate model, alpha =  0.05\n",
      "accuracy:  0.8594\n",
      "class-wise accuracies:  [0.8654443442540828, 0.815597667638484]\n",
      "evaluate model, alpha =  0.1\n",
      "accuracy:  0.8741\n",
      "class-wise accuracies:  [0.88598336839996, 0.7875364431486881]\n",
      "evaluate model, alpha =  0.5\n",
      "accuracy:  0.917\n",
      "class-wise accuracies:  [0.974150886684701, 0.5010932944606414]\n",
      "evaluate model, alpha =  1\n",
      "accuracy:  0.9083\n",
      "class-wise accuracies:  [0.9925859132351468, 0.29518950437317787]\n",
      "evaluate model, alpha =  3\n",
      "accuracy:  0.8888\n",
      "class-wise accuracies:  [0.999499048191564, 0.08381924198250729]\n",
      "evaluate model, alpha =  6\n",
      "accuracy:  0.8824\n",
      "class-wise accuracies:  [0.9997996192766256, 0.02806122448979592]\n",
      "evaluate model, alpha =  10\n",
      "accuracy:  0.8811\n",
      "class-wise accuracies:  [0.9997996192766256, 0.01749271137026239]\n",
      "best model alpha=0.5, evaluating on final test dataset\n",
      "accuracy:  0.8868\n",
      "class-wise accuracies:  [0.9997510456084445, 0.02135774218154081]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8867700167356646"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model_on_dataset(NaiveBayes, wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate model, alpha =  0.01\n",
      "accuracy:  0.5649\n",
      "class-wise accuracies:  [0.5230276888827049, 0.561662198391421, 0.6428571428571429, 0.7357268981753973]\n",
      "evaluate model, alpha =  0.05\n",
      "accuracy:  0.5841\n",
      "class-wise accuracies:  [0.5540559343258662, 0.5428954423592494, 0.6011904761904762, 0.7274867569158329]\n",
      "evaluate model, alpha =  0.1\n",
      "accuracy:  0.6159\n",
      "class-wise accuracies:  [0.5978850702657577, 0.532171581769437, 0.5833333333333334, 0.7321954090641554]\n",
      "evaluate model, alpha =  0.5\n",
      "accuracy:  0.725\n",
      "class-wise accuracies:  [0.802560178099346, 0.26273458445040215, 0.047619047619047616, 0.6668628605061802]\n",
      "evaluate model, alpha =  1\n",
      "accuracy:  0.7599\n",
      "class-wise accuracies:  [0.8925838319187421, 0.08310991957104558, 0.0, 0.5709240729841083]\n",
      "evaluate model, alpha =  3\n",
      "accuracy:  0.7497\n",
      "class-wise accuracies:  [0.9980520384026715, 0.0, 0.0, 0.10241318422601531]\n",
      "evaluate model, alpha =  6\n",
      "accuracy:  0.7334\n",
      "class-wise accuracies:  [1.0, 0.0, 0.0, 0.0]\n",
      "evaluate model, alpha =  10\n",
      "accuracy:  0.7334\n",
      "class-wise accuracies:  [1.0, 0.0, 0.0, 0.0]\n",
      "best model alpha=1, evaluating on final test dataset\n",
      "accuracy:  0.7296\n",
      "class-wise accuracies:  [1.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7295918367346939"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model_on_dataset(NaiveBayes, fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 'not attack':\n",
      "f1 score: 0.993880, precision: 0.9923703677232606, recall: 0.995394\n",
      "label 'attack':\n",
      "f1 score: 0.613122, precision: 0.6783479349186483, recall: 0.559340\n"
     ]
    }
   ],
   "source": [
    "print_f1_scores(NaiveBayes, wiki_data, alpha=0.5, multi_label=False) # best alpha=0.5 (see above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 'unrelated':\n",
      "f1 score: 0.982467, precision: 0.9793938625412123, recall: 0.985559\n",
      "label 'agree':\n",
      "f1 score: 0.241821, precision: 0.7024793388429752, recall: 0.146048\n",
      "label 'disagree':\n",
      "f1 score:   nan, precision:   0.0, recall: 0.000000\n",
      "label 'discuss':\n",
      "f1 score: 0.531117, precision: 0.48110185778347214, recall: 0.592739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnhallman/mlcourse/mlenv/lib/python3.6/site-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "print_f1_scores(NaiveBayes, fake_data, alpha=1.0, multi_label=True) # best alpha=0.5 (see above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43885125"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([0.982467, 0.241821, 0, 0.531117])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slightly more sophisticated model\n",
    "class NaiveBayesBigram(nn.Module):\n",
    "    def __init__(self, vocab, num_labels, train_data, alpha=0.001):\n",
    "        super(NaiveBayesBigram, self).__init__()\n",
    "        self.vocab_len = len(vocab)\n",
    "        self.classes = num_labels\n",
    "        self.p_class = np.zeros(self.classes)\n",
    "        self.p_vocab = {y:{} for y in np.arange(self.classes)} # dict of dicts: second key is bigram tuple\n",
    "        self.class_sum = np.zeros(self.classes) # NOT THE SAME AS P_CLASS\n",
    "        for (x, y) in train_data:\n",
    "            self.p_class[y] += 1\n",
    "            for i in range(len(x)):\n",
    "                if i+1 == len(x) or x[i+1] == 0: break # 0 padding\n",
    "                if (x[i], x[i+1]) not in self.p_vocab[y]: \n",
    "                    self.p_vocab[y][(x[i], x[i+1])] = 0\n",
    "                self.p_vocab[y][(x[i], x[i+1])] += 1\n",
    "                self.class_sum[y] += 1\n",
    "        for y, d in self.p_vocab.items():\n",
    "            for bigram in d.keys():\n",
    "                d[bigram] = (d[bigram] + alpha) / (self.class_sum[y] + self.vocab_len**2 * alpha)\n",
    "        self.p_class /= np.sum(self.p_class)\n",
    "        self.no_show = alpha * np.ones(self.classes) / (self.class_sum + self.vocab_len**2 * alpha)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        log_probs = np.log(self.p_class)\n",
    "        for i in range(len(src)):\n",
    "            if i+1 == len(src) or src[i+1] == 0: break # 0 padding\n",
    "            bigram = (src[i], src[i+1])\n",
    "            scores = [self.p_vocab[y][bigram] if bigram in self.p_vocab[y] else self.no_show[y] \\\n",
    "                      for y in range(self.classes)]\n",
    "            log_probs += np.log(scores)\n",
    "        return np.argmax(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate model, alpha =  0.01\n",
      "accuracy:  0.9006\n",
      "class-wise accuracies:  [0.9979511268802159, 0.1777365491651206]\n",
      "evaluate model, alpha =  0.05\n",
      "accuracy:  0.898\n",
      "class-wise accuracies:  [0.9987007146069662, 0.150278293135436]\n",
      "evaluate model, alpha =  0.1\n",
      "accuracy:  0.8969\n",
      "class-wise accuracies:  [0.999050522212783, 0.13803339517625232]\n",
      "evaluate model, alpha =  0.5\n",
      "accuracy:  0.893\n",
      "class-wise accuracies:  [0.9994503023337165, 0.10241187384044527]\n",
      "evaluate model, alpha =  1\n",
      "accuracy:  0.8917\n",
      "class-wise accuracies:  [0.9996002198790666, 0.09053803339517626]\n",
      "evaluate model, alpha =  3\n",
      "accuracy:  0.8894\n",
      "class-wise accuracies:  [0.9998001099395333, 0.06975881261595547]\n",
      "evaluate model, alpha =  6\n",
      "accuracy:  0.888\n",
      "class-wise accuracies:  [0.9998001099395333, 0.0575139146567718]\n",
      "evaluate model, alpha =  10\n",
      "accuracy:  0.8872\n",
      "class-wise accuracies:  [0.9998500824546499, 0.05083487940630798]\n",
      "best model alpha=0.01, evaluating on final test dataset\n",
      "accuracy:  0.8887\n",
      "class-wise accuracies:  [0.9999001647281984, 0.05499438832772166]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8886637893067911"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model_on_dataset(NaiveBayesBigram, wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate model, alpha =  0.01\n",
      "accuracy:  0.8056\n",
      "class-wise accuracies:  [0.9983379501385041, 0.16137931034482758, 0.04861111111111111, 0.32904734073641145]\n",
      "evaluate model, alpha =  0.05\n",
      "accuracy:  0.8018\n",
      "class-wise accuracies:  [0.9983379501385041, 0.13655172413793104, 0.041666666666666664, 0.3185271770894214]\n",
      "evaluate model, alpha =  0.1\n",
      "accuracy:  0.7984\n",
      "class-wise accuracies:  [0.9983379501385041, 0.11310344827586206, 0.027777777777777776, 0.309760374050263]\n",
      "evaluate model, alpha =  0.5\n",
      "accuracy:  0.7813\n",
      "class-wise accuracies:  [0.9986149584487535, 0.041379310344827586, 0.013888888888888888, 0.24254821741671537]\n",
      "evaluate model, alpha =  1\n",
      "accuracy:  0.7735\n",
      "class-wise accuracies:  [0.9986149584487535, 0.019310344827586208, 0.0, 0.20806545879602573]\n",
      "evaluate model, alpha =  3\n",
      "accuracy:  0.7573\n",
      "class-wise accuracies:  [0.9993074792243767, 0.001379310344827586, 0.0, 0.12039742840444184]\n",
      "evaluate model, alpha =  6\n",
      "accuracy:  0.7481\n",
      "class-wise accuracies:  [1.0, 0.001379310344827586, 0.0, 0.06428988895382817]\n",
      "evaluate model, alpha =  10\n",
      "accuracy:  0.7426\n",
      "class-wise accuracies:  [1.0, 0.001379310344827586, 0.0, 0.03272939801285798]\n",
      "best model alpha=0.01, evaluating on final test dataset\n",
      "accuracy:  0.7432\n",
      "class-wise accuracies:  [1.0, 0.005805515239477504, 0.0, 0.0330818340104469]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7431632653061224"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model_on_dataset(NaiveBayesBigram, fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 'not attack':\n",
      "f1 score: 0.992429, precision: 0.9852413524930625, recall: 0.999722\n",
      "label 'attack':\n",
      "f1 score: 0.288582, precision: 0.9175531914893617, recall: 0.171216\n"
     ]
    }
   ],
   "source": [
    "print_f1_scores(NaiveBayesBigram, wiki_data, alpha=0.01, multi_label=False) # best alpha=0.5 (see above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 'unrelated':\n",
      "f1 score: 0.985487, precision: 0.9714687396625868, recall: 0.999915\n",
      "label 'agree':\n",
      "f1 score: 0.244477, precision: 0.7155172413793104, recall: 0.147425\n",
      "label 'disagree':\n",
      "f1 score: 0.013986, precision:   0.5, recall: 0.007092\n",
      "label 'discuss':\n",
      "f1 score: 0.443344, precision: 0.7840466926070039, recall: 0.309049\n"
     ]
    }
   ],
   "source": [
    "print_f1_scores(NaiveBayesBigram, fake_data, alpha=0.01, multi_label=True) # best alpha=0.5 (see above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4218235"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([0.985487, 0.244477, 0.013986, 0.443344])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD NAIVE BAYES - VERY BAD, DONT USE (runs out of memory)\n",
    "\"\"\"\n",
    "# slightly more sophisticated model\n",
    "class NaiveBayesBigram(nn.Module):\n",
    "    def __init__(self, vocab, num_labels, train_data, alpha=0.001):\n",
    "        super(NaiveBayesBigram, self).__init__()\n",
    "        self.vocab_len = len(vocab)\n",
    "        self.classes = num_labels\n",
    "        self.p_class = np.zeros(self.classes)\n",
    "        self.p_vocab = alpha * np.ones((self.classes, self.vocab_len, self.vocab_len))\n",
    "        print(\"start training\")\n",
    "        for (x, y) in train_data:\n",
    "            self.p_class[y] += 1\n",
    "            for i in range(len(x)):\n",
    "                if i == 0: continue # skip first one\n",
    "                if (x[i+1] == 0): break # 0 padding\n",
    "                self.p_vocab[y, x[i], x[i+1]] += 1\n",
    "        print(\"done with training\")\n",
    "        self.p_class /= np.sum(self.p_class)\n",
    "        for i in range(self.p_vocab.shape[0]):\n",
    "            self.p_vocab[i] = self.p_vocab[i] / np.sum(self.p_vocab[i])\n",
    "        \n",
    "    def forward(self, src):\n",
    "        log_probs = np.log(self.p_class)\n",
    "        for i in range(len(src)):\n",
    "            if (i == 0): continue\n",
    "            if (src[i+1] == 0): break\n",
    "            log_probs += np.log(self.p_vocab[:, src[i], src[i+1]])\n",
    "        return np.argmax(log_probs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# wiki dataset\n",
    "cutoff = 0.3\n",
    "comment_df = pd.read_csv(\"../data/attack_annotated_comments.tsv\", sep ='\\t')\n",
    "comment_df[\"comment\"] = comment_df[\"comment\"].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comment_df[\"comment\"] = comment_df[\"comment\"].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "annotation_df = pd.read_csv(\"../data/attack_annotations.tsv\",  sep='\\t')\n",
    "annotation_df = (annotation_df.groupby(\"rev_id\")[\"attack\"].mean() > cutoff)\n",
    "annotation_df = annotation_df.to_frame().reset_index()\n",
    "final_df = pd.merge(comment_df, annotation_df, how='inner', on=['rev_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iraq is not good  ===  ===  USA is bad   \n",
      "\n",
      "Anon  :What the heck are you talking about? This is an encyclopedia, not a book store. \n",
      "\n",
      "i have a dick, its bigger than yours! hahaha\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(final_df[final_df['attack'] == True]['comment']):\n",
    "    if len(row) < 100:\n",
    "        print(row)\n",
    "        print()\n",
    "    if i > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This page will need disambiguation. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(final_df[final_df['attack'] == False]['comment']):\n",
    "    if len(row) < 100:\n",
    "        print(row)\n",
    "        print()\n",
    "    if i > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake news dataset\n",
    "body_df = pd.read_csv(\"../data/fake_news_bodies.csv\")\n",
    "stance_df = pd.read_csv(\"../data/fake_news_stances.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972\n",
      "115864\n"
     ]
    }
   ],
   "source": [
    "print(len(stance_df))\n",
    "print(len(comment_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
